{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Stanford Dog Breed 데이터 세트를 아래 URL에서 직접 Download 및 압축 해제\n* Kaggle의 Dataset으로 Object Storage 연결 시 이미지를 한장 씩 읽는 데 많은 시간이 소요되어 모델 학습에 시간이 더 걸림. \n* Local Disk에 바로 이미지를 다운로드/압축 해제 후 모델에서 이를 이용할 수 있도록 함. ","metadata":{}},{"cell_type":"code","source":"# stanford dog breed 데이터 세트 다운로드 \n!wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n# 현재 디렉토리인 /kaggle/working에 바로 압축 해제 \n!ls; tar -xvf images.tar","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pretrained 모델 생성. \n* resnet50, xception, efficientnetb0, efficientnetb1 등으로 pretrained 모델을 생성할 수 있는 함수 생성. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential , Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\nfrom tensorflow.keras.applications import Xception, ResNet50V2, EfficientNetB0, EfficientNetB1\nfrom tensorflow.keras.applications import MobileNet\nimport tensorflow as tf\n\n# dog breed 종류는 120가지\n\ndef create_model(model_type='xception', in_shape = (224, 224, 3), n_classes = 120):\n    input_tensor = Input(shape = in_shape)\n    if model_type == 'resnet50v2':\n        base_model = tf.keras.applications.ResNet50V2(include_top = False, weights = 'imagenet', input_tensor = input_tensor)\n    elif model_type == 'xception':\n        base_model = tf.keras.applications.Xception(include_top = False, weights = 'imagenet', input_tensor = input_tensor)\n    elif model_type == 'efficientnetb0':\n        base_model = tf.keras.applications.EfficientNetB0(include_top = False, weights = 'imagenet', input_tensor = input_tensor)\n    elif model_type == 'efficientnetb1':\n        base_model = tf.keras.applications.EfficientNetB1(include_top = False, weights = 'imagenet', input_tensor = input_tensor)\n        \n    x = base_model.output  \n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation = 'relu')(x)\n    x = Dropout(0.5)(x)    \n    preds = Dense(units = n_classes, activation = 'softmax')(x)\n    model = Model(inputs = input_tensor, outputs = preds)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-02T09:44:43.900689Z","iopub.execute_input":"2022-02-02T09:44:43.900958Z","iopub.status.idle":"2022-02-02T09:44:49.152848Z","shell.execute_reply.started":"2022-02-02T09:44:43.900928Z","shell.execute_reply":"2022-02-02T09:44:49.152065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습/검증 데이터 분할, Dataset 생성, 모델 생성, 모델 Opt, Loss설정, Learning Rate Callback 설정 함수 생성. \n* Prtrained 모델 유형, 메타 DataFrame, 초기 학습율, Augmentor, scaling 함수를 인자로 입력. \n* Learning Rate Scheduler는 ReduceLROnPlateau 적용. ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import Sequence\nimport sklearn \nimport cv2\n\nimport albumentations as A\n\nIMAGE_DIR = '/kaggle/working/Images' \n\ndef make_dogbreed_dataframe(image_dir = IMAGE_DIR):\n    paths = []\n    label_distinguish = []\n    for dirname, _, filenames in os.walk(image_dir):\n        for filename in filenames:\n            # 이미지 파일이 아닌 파일도 해당 디렉토리에 있음.\n            if '.jpg' in filename:\n                # 파일의 절대 경로를 file_path 변수에 할당. \n                file_path = dirname + '/' + filename\n                paths.append(file_path)\n                # 이미지 파일의 절대 경로에서 레이블명 생성을 위한 1차 추출. '/'로 분할하여 파일 바로 위 서브디렉토리 이름 가져옴.  \n                start_pos = file_path.find('/', 20)\n                end_pos = file_path.rfind('/')\n                tmp_breed = file_path[start_pos + 1 : end_pos]\n                # 1차 추출된 데이터를 기반으로 '-' 이후 데이터가 레이블 값임. \n                breed = tmp_breed[tmp_breed.find('-') + 1 :]\n                #print(start_pos, end_pos, tmp_breed)\n                label_distinguish.append(breed)\n\n    data_df = pd.DataFrame({'path':paths, 'label':label_distinguish})\n    return data_df\n\n# 학습과 검증 데이터용 numpy array 분리. \ndef get_train_valid(train_df, valid_size = 0.2, random_state = 2021):\n    train_path = train_df['path'].values\n    train_label = pd.get_dummies(train_df['label']).values\n    \n    tr_path, val_path, tr_label, val_label = train_test_split(train_path, train_label, test_size = valid_size, random_state = random_state)\n    print('tr_path shape:', tr_path.shape, 'tr_label shape:', tr_label.shape, 'val_path shape:', val_path.shape, 'val_label shape:', val_label.shape)\n    return tr_path, val_path, tr_label, val_label\n\n\nBATCH_SIZE = 64\nIMAGE_SIZE = 224\n\n# 입력 인자 image_filenames, labels는 모두 numpy array로 들어옴. \nclass Breed_Dataset(Sequence):\n    def __init__(self, image_filenames, labels, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                 augmentor = None, shuffle = False, pre_func = None):\n        '''\n        파라미터 설명\n        image_filenames: opencv로 image를 로드할 파일의 절대 경로들\n        labels: 해당 image의 label들\n        batch_size: __getitem__(self, index) 호출 시 마다 가져올 데이터 batch 건수\n        augmentor: albumentations 객체\n        shuffle: 학습 데이터의 경우 epoch 종료시마다 데이터를 섞을지 여부\n        '''\n        # 객체 생성 인자로 들어온 값을 객체 내부 변수로 할당. \n        self.image_filenames = image_filenames\n        self.labels = labels\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.augmentor = augmentor\n        self.pre_func = pre_func\n        # train data의 경우 \n        self.shuffle = shuffle\n        if self.shuffle:\n            # 객체 생성시에 한번 데이터를 섞음. \n            #self.on_epoch_end()\n            pass\n    \n    # Sequence를 상속받은 Dataset은 batch_size 단위로 입력된 데이터를 처리함. \n    # __len__()은 전체 데이터 건수가 주어졌을 때 batch_size단위로 몇번 데이터를 반환하는지 나타남\n    def __len__(self):\n        # batch_size단위로 데이터를 몇번 가져와야하는지 계산하기 위해 전체 데이터 건수를 batch_size로 나누되, 정수로 정확히 나눠지지 않을 경우 1회를 더한다. \n        return int(np.ceil(len(self.labels) / self.batch_size))\n    \n    # batch_size 단위로 image_array, label_array 데이터를 가져와서 변환한 뒤 다시 반환함\n    # 인자로 몇번째 batch 인지를 나타내는 index를 입력하면 해당 순서에 해당하는 batch_size 만큼의 데이타를 가공하여 반환\n    # batch_size 갯수만큼 변환된 image_array와 label_array 반환. \n    def __getitem__(self, index):\n        # index는 몇번째 batch인지를 나타냄. \n        # batch_size만큼 순차적으로 데이터를 가져오려면 array에서 index * self.batch_size : (index + 1) * self.batch_size 만큼의 연속 데이터를 가져오면 됨\n        image_name_batch = self.image_filenames[index * self.batch_size : (index + 1) * self.batch_size]\n        if self.labels is not None:\n            label_batch = self.labels[index * self.batch_size : (index + 1) * self.batch_size]\n        \n        # 만일 객체 생성 인자로 albumentation으로 만든 augmentor가 주어진다면 아래와 같이 augmentor를 이용하여 image 변환\n        # albumentations은 개별 image만 변환할 수 있으므로 batch_size만큼 할당된 image_name_batch를 한 건씩 iteration하면서 변환 수행. \n        # image_batch 배열은 float32 로 설정. \n        image_batch = np.zeros((image_name_batch.shape[0], self.image_size, self.image_size, 3), dtype = 'float32')\n        \n        # batch_size에 담긴 건수만큼 iteration 하면서 opencv image load -> image augmentation 변환(augmentor가 not None일 경우)-> image_batch에 담음. \n        for image_index in range(image_name_batch.shape[0]):\n            image = cv2.cvtColor(cv2.imread(image_name_batch[image_index]), cv2.COLOR_BGR2RGB)\n            if self.augmentor is not None:\n                image = self.augmentor(image=image)['image']\n            #crop 시 잘린 이미지가 원본 이미지와 다르게 되므로 augmentation 적용 후 resize() 적용. \n            image = cv2.resize(image, (self.image_size, self.image_size))\n            # 만일 preprocessing_input이 pre_func인자로 들어오면 이를 이용하여 scaling 적용. \n            if self.pre_func is not None:\n                image = self.pre_func(image)\n                \n            image_batch[image_index] = image\n        \n        return image_batch, label_batch\n    \n    # epoch가 한번 수행이 완료 될 때마다 모델의 fit()에서 호출됨. \n    def on_epoch_end(self):\n        if(self.shuffle):\n            #print('epoch end')\n            # 전체 image 파일의 위치와 label를 쌍을 맞춰서 섞어준다. scikt learn의 utils.shuffle에서 해당 기능 제공\n            self.image_filenames, self.labels = sklearn.utils.shuffle(self.image_filenames, self.labels)\n        else:\n            pass\n        \n        \naugmentor_light = A.Compose([\n    A.HorizontalFlip(p=0.5),\n])\n\n\ndata_df = make_dogbreed_dataframe(image_dir = IMAGE_DIR)\ntrain_df, test_df = train_test_split(data_df, test_size = 0.4, stratify = data_df['label'], random_state = 2021)\nprint(train_df.shape, test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T09:44:53.168939Z","iopub.execute_input":"2022-02-02T09:44:53.169216Z","iopub.status.idle":"2022-02-02T09:44:55.008534Z","shell.execute_reply.started":"2022-02-02T09:44:53.169181Z","shell.execute_reply":"2022-02-02T09:44:55.00775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 30\nBATCH_SIZE = 64\nIMAGE_SIZE = 224\n\ndef train_model(model_type, train_df, initial_lr = 0.001, augmentor = None, input_pre_func = None):\n    tr_path, val_path, tr_label, val_label = get_train_valid(train_df, valid_size = 0.2, random_state = 2021)\n    \n    tr_ds = Breed_Dataset(tr_path, tr_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                          augmentor = augmentor, shuffle = True, pre_func = input_pre_func)\n    val_ds = Breed_Dataset(val_path, val_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE,  \n                          augmentor = None, shuffle = False, pre_func = input_pre_func)\n\n    # model_type인자로 들어온 모델 생성. optimizer Adam적용. \n    print('#######', model_type, ' 생성 및 학습 수행 ########')\n    model = create_model(model_type = model_type)\n    model.compile(optimizer = Adam(lr = initial_lr), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n    # 3번 iteration내에 validation loss가 향상되지 않으면 learning rate을 기존 learning rate * 0.2로 줄임.  \n    rlr_cb = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, mode = 'min', verbose = 1)\n    # 10번 iteration내에 validation loss가 향상되지 않으면 더 이상 학습하지 않고 종료\n    ely_cb = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', verbose = 1)\n    \n    history = model.fit(tr_ds, epochs = N_EPOCHS, steps_per_epoch = int(np.ceil(tr_path.shape[0] / BATCH_SIZE)), \n                   validation_data = val_ds, validation_steps = int(np.ceil(val_path.shape[0] / BATCH_SIZE)),\n                   callbacks = ([rlr_cb, ely_cb]), verbose = 1)\n    \n    return model, history\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T09:45:01.180825Z","iopub.execute_input":"2022-02-02T09:45:01.1811Z","iopub.status.idle":"2022-02-02T09:45:01.191027Z","shell.execute_reply.started":"2022-02-02T09:45:01.181068Z","shell.execute_reply":"2022-02-02T09:45:01.190132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimport seaborn as sns\n%matplotlib inline \n\ndef show_grid_images(image_path_list, augmentor = None, ncols = 4, title = None):\n    figure, axs = plt.subplots(figsize = (22, 4), nrows = 1, ncols = ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]), cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (224, 224))\n        if augmentor is not None:\n            image = augmentor(image = image)['image']\n        axs[i].imshow(image)\n        axs[i].axis('off')\n        axs[i].set_title(title) \n        \nbreed_image_list_01 = data_df[data_df['label'] == 'Siberian_husky']['path'].iloc[:6].tolist()\nbreed_image_list_02 = data_df[data_df['label'] == 'Eskimo_dog']['path'].iloc[:6].tolist()\n\nshow_grid_images(breed_image_list_01, ncols = 6, title = 'Siberian_husky')\nshow_grid_images(breed_image_list_02, ncols = 6, title = 'Eskimo_dog')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T09:45:04.812363Z","iopub.execute_input":"2022-02-02T09:45:04.812783Z","iopub.status.idle":"2022-02-02T09:45:06.032539Z","shell.execute_reply.started":"2022-02-02T09:45:04.812748Z","shell.execute_reply":"2022-02-02T09:45:06.031215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EfficientNetB0 기반에서 Augmentation 기법을 변화 시키면서 모델 학습. ReduceLROnPlateau 적용. \n* 학습 데이터가 적을 경우 Augmentation이 너무 약할 경우, 과적합(Overfitting), Augmentation이 너무 강하거나 잘못 될 경우 과소적합(Underfitting)이 가능성이 있음. \n* 이전에 적용한 좌우 반전보다는 더 다양한 기법을 적용하면서 모델 학습하고, 학습데이터와 검증 데이터의 loss/metric 변화 추이 모니터링. ","metadata":{}},{"cell_type":"code","source":"augmentor_heavy_01 = A.Compose([\n    A.HorizontalFlip(p = 0.5),\n    A.ShiftScaleRotate(p = 0.2),\n    A.CenterCrop(height = 90, width = 90, p = 0.2),\n    A.RandomBrightnessContrast(brightness_limit = (-0.2, 0.2), contrast_limit = (-0.2, 0.2), p = 0.2),\n    A.ColorJitter(p = 0.2),\n    A.OneOf(\n        [A.CoarseDropout(p = 1, max_holes = 26), \n         A.CLAHE(p = 1),\n         A.Blur(blur_limit = (10, 15), p = 1)\n        ], p = 0.3)\n])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T09:45:09.530845Z","iopub.execute_input":"2022-02-02T09:45:09.531105Z","iopub.status.idle":"2022-02-02T09:45:09.537077Z","shell.execute_reply.started":"2022-02-02T09:45:09.531072Z","shell.execute_reply":"2022-02-02T09:45:09.536355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breed_image_list_01 = data_df[data_df['label'] == 'Staffordshire_bullterrier']['path'].iloc[:6].tolist()       \nshow_grid_images(breed_image_list_01, augmentor = None, ncols = 6, title = 'orignal Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_01, augmentor = augmentor_heavy_01, ncols = 6, title = 'augmented')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T09:45:13.646115Z","iopub.execute_input":"2022-02-02T09:45:13.646681Z","iopub.status.idle":"2022-02-02T09:45:14.700629Z","shell.execute_reply.started":"2022-02-02T09:45:13.646639Z","shell.execute_reply":"2022-02-02T09:45:14.699977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\n\neffb0_model_aug01, effb0_history_aug01 = train_model(model_type='efficientnetb0', train_df = train_df, initial_lr = 0.0001, \n                                                     augmentor = augmentor_heavy_01, input_pre_func = eff_preprocess_input)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T09:45:19.651239Z","iopub.execute_input":"2022-02-02T09:45:19.651502Z","iopub.status.idle":"2022-02-02T10:33:24.1625Z","shell.execute_reply.started":"2022-02-02T09:45:19.651469Z","shell.execute_reply":"2022-02-02T10:33:24.161718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 학습된 모델을 이용하여 테스트 데이터로 Evaluation 및 Prediction 수행. ","metadata":{}},{"cell_type":"code","source":"test_path = test_df['path'].values\ntest_label = pd.get_dummies(test_df['label']).values\ntest_classes = np.argmax(test_label, axis = 1)\ntest_df['gt_class'] = test_classes\n\ntest_ds = Breed_Dataset(test_path, test_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                        augmentor = None, shuffle = False, pre_func = eff_preprocess_input)\n\neffb0_model_aug01.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:33:43.29426Z","iopub.execute_input":"2022-02-02T10:33:43.294739Z","iopub.status.idle":"2022-02-02T10:34:24.554838Z","shell.execute_reply.started":"2022-02-02T10:33:43.2947Z","shell.execute_reply":"2022-02-02T10:34:24.55408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 테스트 Dataset으로 개별 image들의 predict 수행. \npredict_result = effb0_model_aug01.predict(test_ds, steps = int(np.ceil(len(test_label) / BATCH_SIZE)))\npredict_class = np.argmax(predict_result, axis = 1)\n\ntest_df['effb0_aug01_pred_class'] = predict_class\nprint(test_df[test_df['gt_class'] != test_df['effb0_aug01_pred_class']]['label'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 다른 Augmentation을 적용. \n* CenterCrop 제외하고 probability를 약간 변경.  ","metadata":{}},{"cell_type":"code","source":"augmentor_heavy_02 = A.Compose([\n    A.HorizontalFlip(p = 0.5),\n    A.ShiftScaleRotate(p = 0.3),\n    A.RandomBrightnessContrast(brightness_limit = (-0.2, 0.2), contrast_limit = (-0.2, 0.2), p = 0.3),\n    A.ColorJitter(p = 0.3),\n    A.OneOf(\n        [A.CoarseDropout(p = 0.3, max_holes = 26), \n         A.CLAHE(p = 0.3),\n         A.Blur(blur_limit = (10, 15), p = 0.3)\n        ], p = 0.3)\n])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:35:10.843254Z","iopub.execute_input":"2022-02-02T10:35:10.84353Z","iopub.status.idle":"2022-02-02T10:35:10.849551Z","shell.execute_reply.started":"2022-02-02T10:35:10.8435Z","shell.execute_reply":"2022-02-02T10:35:10.848667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breed_image_list_01 = data_df[data_df['label'] == 'Staffordshire_bullterrier']['path'].iloc[:6].tolist()       \nshow_grid_images(breed_image_list_01, augmentor = None, ncols = 6, title = 'orignal Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_01, augmentor = augmentor_heavy_02, ncols = 6, title = 'augmented Staffordshire_bullterrier')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T10:35:14.354689Z","iopub.execute_input":"2022-02-02T10:35:14.35524Z","iopub.status.idle":"2022-02-02T10:35:15.490621Z","shell.execute_reply.started":"2022-02-02T10:35:14.355204Z","shell.execute_reply":"2022-02-02T10:35:15.489998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\n\neffb0_model_aug02, effb0_history_aug02 = train_model(model_type = 'efficientnetb0', train_df = train_df, initial_lr = 0.0001,\n                                               augmentor = augmentor_heavy_02, input_pre_func = eff_preprocess_input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = test_df['path'].values\ntest_label = pd.get_dummies(test_df['label']).values\n\n\ntest_ds = Breed_Dataset(test_path, test_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                        augmentor = None, shuffle = False, pre_func = eff_preprocess_input)\n\neffb0_model_aug02.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T11:26:34.836488Z","iopub.execute_input":"2022-02-02T11:26:34.836744Z","iopub.status.idle":"2022-02-02T11:27:13.421089Z","shell.execute_reply.started":"2022-02-02T11:26:34.836715Z","shell.execute_reply":"2022-02-02T11:27:13.420471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Learning Rate Scheduler를 Ramp Up and Step Decay 방식으로 변경\n* 최초는 1e-5에서 2회 Ramp up 단계를 거쳐서, Max인 1e-4까지 증가 시킴. 이후는 Step Decay 방식으로 2회 마다 learning rate를 줄임. ","metadata":{}},{"cell_type":"code","source":"# learning rate scheduler에 적용할 함수 선언. 내포 함수를 사용. \ndef lrfn(epoch):\n    # 내포 함수인 calc_fn()에서 사용되는 파라미터. \n    LR_START = 1e-5\n    LR_MAX = 1e-4\n    LR_RAMPUP_EPOCHS = 2\n    LR_SUSTAIN_EPOCHS = 2\n    LR_STEP_DECAY = 0.75\n    \n    def calc_fn(epoch):\n        if epoch < LR_RAMPUP_EPOCHS:\n            lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n            lr = LR_MAX\n        else:\n            lr = LR_MAX * LR_STEP_DECAY ** ((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) // 2)\n        return lr\n    \n    # 반드시 내포 함수인 calc_fn(epoch)를 호출해야함. \n    return calc_fn(epoch)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T11:27:45.707925Z","iopub.execute_input":"2022-02-02T11:27:45.708207Z","iopub.status.idle":"2022-02-02T11:27:45.714064Z","shell.execute_reply.started":"2022-02-02T11:27:45.708172Z","shell.execute_reply":"2022-02-02T11:27:45.713327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(30):\n    print(lrfn(i + 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 기존 train_model()에 RampUp and Step Decay를 Callback으로 반영할 수 있도록 train_model_with_aug_lr()로 함수 수정.\n* Learning Rate Scheduler등의 Callback 객체를 인자로 입력 받을 수 있도록 함수 수정. ","metadata":{}},{"cell_type":"code","source":"N_EPOCHS = 30\nBATCH_SIZE = 64\nIMAGE_SIZE = 224\n\ndef train_model_with_aug_lr(model_type, train_df, initial_lr = 0.001, augmentor = None, callbacks_list = None, input_pre_func = None):\n    \n    tr_path, val_path, tr_label, val_label = get_train_valid(train_df, valid_size = 0.2, random_state = 2021)\n    \n    tr_ds = Breed_Dataset(tr_path, tr_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                          augmentor = augmentor, shuffle = True, pre_func = input_pre_func)\n    val_ds = Breed_Dataset(val_path, val_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                          augmentor = None, shuffle = False, pre_func = input_pre_func)\n\n    # model_type인자로 들어온 모델 생성. optimizer Adam적용. \n    print('#######', model_type, ' 생성 및 학습 수행 ########')\n    model = create_model(model_type = model_type)\n    model.compile(optimizer = Adam(lr = initial_lr), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    # learning rate scheduler와 early stopping 을 함수 인자로 입력 받음. \n    history = model.fit(tr_ds, epochs = N_EPOCHS, steps_per_epoch = tr_path.shape[0] // BATCH_SIZE, \n                   validation_data = val_ds, validation_steps = val_path.shape[0] // BATCH_SIZE,\n                   callbacks = (callbacks_list), verbose = 1)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2022-02-02T11:27:53.885168Z","iopub.execute_input":"2022-02-02T11:27:53.88572Z","iopub.status.idle":"2022-02-02T11:27:53.896786Z","shell.execute_reply.started":"2022-02-02T11:27:53.885678Z","shell.execute_reply":"2022-02-02T11:27:53.896015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\nimport tensorflow as tf\n\n# Learning Rate Scheduler(Ramp up and step down decay) 와 Early Stopping callback 생성. \nlr_cb = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = 1) # fit을 할 때 콜백 호출이 되는데, 그 때 현재 몇번째 epoch를 돌고 있는지를 인자로 전달.\nely_cb = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', verbose = 1)\n# train_model에 인자로 넣을 Callback 객체의 리스트 생성. \ncallbacks_list = [lr_cb, ely_cb]\n\naugmentor_heavy_01 = A.Compose([\n    A.HorizontalFlip(p = 0.5),\n    A.ShiftScaleRotate(p = 0.2),\n    A.CenterCrop(height = 90, width = 90, p = 0.2),\n    A.RandomBrightnessContrast(brightness_limit = (-0.2, 0.2), contrast_limit = (-0.2, 0.2), p = 0.2),\n    A.ColorJitter(p = 0.2),\n    A.OneOf(\n        [A.CoarseDropout(p = 1, max_holes = 26), \n         A.CLAHE(p = 1),\n         A.Blur(blur_limit = (10, 15), p = 1)\n        ], p = 0.3)\n])\n\naugmentor_heavy_02 = A.Compose([\n    A.HorizontalFlip(p = 0.5),\n    A.ShiftScaleRotate(p = 0.3),\n    A.RandomBrightnessContrast(brightness_limit = (-0.2, 0.2), contrast_limit = (-0.2, 0.2), p = 0.3),\n    A.ColorJitter(p = 0.3),\n    A.OneOf(\n        [A.CoarseDropout(p = 0.3, max_holes = 26), \n         A.CLAHE(p = 0.3),\n         A.Blur(blur_limit = (10, 15), p = 0.3)\n        ], p = 0.3)\n])\n\n# augmentor_heavy_01을 ramp up and step decay 적용. \neffb0_model_lr01, effb0_history_lr01 = train_model_with_aug_lr(model_type = 'efficientnetb0', train_df = train_df, initial_lr = 0.0001, \n                                               augmentor = augmentor_heavy_01, callbacks_list = callbacks_list, input_pre_func = eff_preprocess_input)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T11:27:58.807983Z","iopub.execute_input":"2022-02-02T11:27:58.808356Z","iopub.status.idle":"2022-02-02T12:30:46.16207Z","shell.execute_reply.started":"2022-02-02T11:27:58.808318Z","shell.execute_reply":"2022-02-02T12:30:46.161339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = test_df['path'].values\ntest_label = pd.get_dummies(test_df['label']).values\n\ntest_ds = Breed_Dataset(test_path, test_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                        augmentor = None, shuffle = False, pre_func = eff_preprocess_input)\n\neffb0_model_lr01.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T12:36:38.913819Z","iopub.execute_input":"2022-02-02T12:36:38.914402Z","iopub.status.idle":"2022-02-02T12:37:20.17724Z","shell.execute_reply.started":"2022-02-02T12:36:38.91436Z","shell.execute_reply":"2022-02-02T12:37:20.176557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\nimport tensorflow as tf\n\n# augmentor_heavy_02를 ramp up and step decay 적용. \neffb0_model_lr02, effb0_history_lr02 = train_model_with_aug_lr(model_type = 'efficientnetb0', train_df = train_df, initial_lr = 0.0001, \n                                               augmentor = augmentor_heavy_02, callbacks_list = callbacks_list, input_pre_func = eff_preprocess_input)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T12:37:22.981596Z","iopub.execute_input":"2022-02-02T12:37:22.98184Z","iopub.status.idle":"2022-02-02T13:41:28.956195Z","shell.execute_reply.started":"2022-02-02T12:37:22.981812Z","shell.execute_reply":"2022-02-02T13:41:28.955404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = Breed_Dataset(test_path, test_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                        augmentor = None, shuffle = False, pre_func = eff_preprocess_input)\n\neffb0_model_lr02.evaluate(test_ds)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:41:38.677593Z","iopub.execute_input":"2022-02-02T13:41:38.677844Z","iopub.status.idle":"2022-02-02T13:42:19.900889Z","shell.execute_reply.started":"2022-02-02T13:41:38.677817Z","shell.execute_reply":"2022-02-02T13:42:19.900057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pretrained 모델의 Fine Tuning 적용. \n* Fine tuning으로 1차 dense layer 부터 학습 적용, 2차 전체 Layer 학습 적용. \n* EfficientNet의 경우는 Batch Normalization은 학습하지 않도록 설정. ","metadata":{}},{"cell_type":"code","source":"model_tmp = create_model(model_type = 'efficientnetb0')\nmodel_tmp.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\nimport tensorflow as tf\n\nN_EPOCHS = 30\nBATCH_SIZE = 64\nIMAGE_SIZE = 224\n\ndef train_model_with_ft(model_type, train_df, initial_lr = 0.0001, augmentor = None, callbacks_list = None, input_pre_func = None):\n    \n    tr_path, val_path, tr_label, val_label = get_train_valid(train_df, valid_size = 0.2, random_state = 2021)\n\n    tr_ds = Breed_Dataset(tr_path, tr_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                        augmentor = augmentor, shuffle = True, pre_func = input_pre_func)\n    val_ds = Breed_Dataset(val_path, val_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                        augmentor = None, shuffle = False, pre_func = input_pre_func)\n\n    # model_type인자로 들어온 모델 생성. optimizer Adam적용. \n    print('#######', model_type, ' 생성 및 학습 수행 ########')\n    model = create_model(model_type = model_type)\n    \n    # Feature Extractor layer들을 모두 Freeze\n    for layer in model.layers[:-4]:\n        layer.trainable = False\n    \n    model.compile(optimizer = Adam(lr = initial_lr), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    #10번 iteration내에 validation loss가 향상되지 않으면 더 이상 학습하지 않고 종료\n    ely_cb = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', verbose = 1)\n    #cosine_decay = tf.keras.experimental.CosineDecay(initial_learning_rate = 0.001, decay_steps = 300)\n    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = 1)\n    \n    ### Feature Extractor layer들은 학습하지 않고 Dense Layer만 일차 학습. \n    print('##### Feature Extractor freeze후 Dense layer 학습 시작 ##### ')\n    history = model.fit(tr_ds, epochs = 15, steps_per_epoch = int(np.ceil(tr_path.shape[0] / BATCH_SIZE)), \n                  validation_data = val_ds, validation_steps = int(np.ceil(val_path.shape[0] / BATCH_SIZE)),\n                  callbacks = (callbacks_list), verbose = 1)\n    # efficientNet의 일부만 trainable 가능하게 설정. 특히 BatchNormalization layer는 trainable False로 유지. \n    # https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n    #for layer in model.layers[-20:]:\n    for layer in model.layers:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n    \n    print('##### 전체 Layer Unfreeze 후 학습 시작 ##### ')\n    history = model.fit(tr_ds, epochs = 25, steps_per_epoch = int(np.ceil(tr_path.shape[0] / BATCH_SIZE)), \n                  validation_data = val_ds, validation_steps = int(np.ceil(val_path.shape[0] / BATCH_SIZE)),\n                  callbacks = (callbacks_list), verbose = 1)\n\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:43:15.459779Z","iopub.execute_input":"2022-02-02T13:43:15.460041Z","iopub.status.idle":"2022-02-02T13:43:15.474714Z","shell.execute_reply.started":"2022-02-02T13:43:15.460011Z","shell.execute_reply":"2022-02-02T13:43:15.47395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\nimport tensorflow as tf\n\n# learning rate scheduler에 적용할 함수 선언. 내포 함수를 사용. \ndef lrfn(epoch):\n    # 내포 함수인 calc_fn()에서 사용되는 파라미터. \n    LR_START = 1e-5\n    LR_MAX = 1e-4\n    LR_RAMPUP_EPOCHS = 2\n    LR_SUSTAIN_EPOCHS = 1\n    LR_STEP_DECAY = 0.75\n    \n    def calc_fn(epoch):\n        if epoch < LR_RAMPUP_EPOCHS:\n            lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n            lr = LR_MAX\n        else:\n            lr = LR_MAX * LR_STEP_DECAY ** ((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) // 2)\n        return lr\n    \n    # 반드시 내포 함수인 calc_fn(epoch)를 호출해야함. \n    return calc_fn(epoch)\n\n# Learning Rate Scheduler(Ramp up and step down decay) 와 Early Stopping callback 생성. \nlr_cb = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = 1)\nely_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, mode = 'min', verbose = 1)\n# train_model에 인자로 넣을 Callback 객체의 리스트 생성. \ncallbacks_list = [lr_cb, ely_cb]\n\n\naugmentor_light_01 = A.Compose([\n    A.HorizontalFlip(p = 0.5),\n])\n\naugmentor_light_02 = A.Compose([\n    A.HorizontalFlip(p = 0.3),\n    A.ShiftScaleRotate(scale_limit = (0.7, 0.9), p = 0.2, rotate_limit = 30),\n    A.RandomBrightnessContrast(brightness_limit = (-0.2, 0.2), contrast_limit = (-0.2, 0.2), p = 0.2),\n    A.ColorJitter(p = 0.2)\n])\n\naugmentor_heavy_01 = A.Compose([\n    A.HorizontalFlip(p = 0.5),\n    A.ShiftScaleRotate(p = 0.2),\n    A.CenterCrop(height = 90, width = 90, p = 0.2),\n    A.RandomBrightnessContrast(brightness_limit = (-0.2, 0.2), contrast_limit = (-0.2, 0.2), p = 0.2),\n    A.ColorJitter(p = 0.2),\n    A.OneOf(\n        [A.CoarseDropout(p = 1, max_holes = 26), \n         A.CLAHE(p = 1),\n         A.Blur(blur_limit = (10, 15), p = 1)\n        ], p = 0.3)\n])\n\naugmentor_heavy_02 = A.Compose([\n    A.HorizontalFlip(p = 0.5),\n    A.ShiftScaleRotate(p = 0.3),\n    A.RandomBrightnessContrast(brightness_limit = (-0.2, 0.2), contrast_limit = (-0.2, 0.2), p = 0.3),\n    A.ColorJitter(p = 0.3),\n    A.OneOf(\n        [A.CoarseDropout(p = 0.3, max_holes = 26), \n         A.CLAHE(p = 0.3),\n         A.Blur(blur_limit = (10, 15), p = 0.3)\n        ], p = 0.3)\n])","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:43:24.185362Z","iopub.execute_input":"2022-02-02T13:43:24.185818Z","iopub.status.idle":"2022-02-02T13:43:24.201312Z","shell.execute_reply.started":"2022-02-02T13:43:24.185781Z","shell.execute_reply":"2022-02-02T13:43:24.200565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effb0_model_ft01, effb0_history_ft01 = train_model_with_ft(model_type = 'efficientnetb0', train_df = train_df, initial_lr = 0.0001, \n                                            augmentor = augmentor_heavy_01, callbacks_list = callbacks_list, input_pre_func = eff_preprocess_input)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T13:43:35.849891Z","iopub.execute_input":"2022-02-02T13:43:35.850659Z","iopub.status.idle":"2022-02-02T14:32:26.948493Z","shell.execute_reply.started":"2022-02-02T13:43:35.850614Z","shell.execute_reply":"2022-02-02T14:32:26.947726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = test_df['path'].values\ntest_label = pd.get_dummies(test_df['label']).values\n\ntest_ds = Breed_Dataset(test_path, test_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                        augmentor = None, shuffle = False, pre_func = eff_preprocess_input)\n\neffb0_model_ft01.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T14:32:42.399751Z","iopub.execute_input":"2022-02-02T14:32:42.400007Z","iopub.status.idle":"2022-02-02T14:33:21.13278Z","shell.execute_reply.started":"2022-02-02T14:32:42.399975Z","shell.execute_reply":"2022-02-02T14:33:21.132083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### lighter한 Augmentation 적용. ","metadata":{}},{"cell_type":"code","source":"effb0_model_ft02, effb0_history_ft02 = train_model_with_ft(model_type = 'efficientnetb0', train_df = train_df, initial_lr = 0.0001, \n                                            augmentor = augmentor_light_02, callbacks_list = callbacks_list, input_pre_func = eff_preprocess_input)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T14:33:43.993586Z","iopub.execute_input":"2022-02-02T14:33:43.99394Z","iopub.status.idle":"2022-02-02T15:18:50.070365Z","shell.execute_reply.started":"2022-02-02T14:33:43.993895Z","shell.execute_reply":"2022-02-02T15:18:50.069601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = Breed_Dataset(test_path, test_label, image_size = IMAGE_SIZE, batch_size = BATCH_SIZE, \n                        augmentor = None, shuffle = False, pre_func = eff_preprocess_input)\n\neffb0_model_ft02.evaluate(test_ds)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:25:24.553344Z","iopub.execute_input":"2022-02-02T15:25:24.554131Z","iopub.status.idle":"2022-02-02T15:26:03.107843Z","shell.execute_reply.started":"2022-02-02T15:25:24.554081Z","shell.execute_reply":"2022-02-02T15:26:03.107011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 지금까지 변경 테스트한 여러 환경들을 손쉽게 테스트 해볼 수 있도록 함수 재구성. \n* Config 클래스를 만들어서 여기에 테스트에 필요한 인자들을 모두 설정 할 수 있도록 함. \n* train_model() 인자로 Config 를 입력 받아서 이를 기반으로 학습을 수행할 수 있도록 변경. ","metadata":{}},{"cell_type":"code","source":"class Config:\n    MODEL_TYPE = 'effcientnetb0'\n    IMAGE_SIZE = 224\n    BATCH_SIZE = 64\n    N_EPOCHS = 30 # fine tuning이 아닐 경우 전체 수행 epoch 횟수\n    IS_FINE_TUNING = False # Fine Tuning 여부\n    FIRST_EPOCHS = 15 # fine tuning 일 경우 첫번째 epoch 횟수\n    SECOND_EPOCHS = 25 # fine tuning 일 경우 두번째 epoch 횟수\n    FIRST_CALLBACKS = None # 모델 train시 적용될 callback 객체들의 List -> imagenet weight가 잘 적용되는 경우, first에서 lr을 크게 가져가고 두번째에서 줄이는 방식이 성능이 더 좋다.\n    SECOND_CALLBACKS = None # 만일 Fine tuning 시 첫번째 학습과 두번째 학습의 Learning rate scheduler가 서로 다를 경우 사용. \n    AUGMENTOR = None\n    PRE_FUNC = None\n    INITIAL_LR = 0.0001 # Optimizer에 적용될 최초 Learning rate\n    DEBUG = False","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:26:06.751812Z","iopub.execute_input":"2022-02-02T15:26:06.752519Z","iopub.status.idle":"2022-02-02T15:26:06.759251Z","shell.execute_reply.started":"2022-02-02T15:26:06.752475Z","shell.execute_reply":"2022-02-02T15:26:06.758219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 아래는 기존에 사용한 라이브러리 함수를 그대로 가져온 것임","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential , Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\nfrom tensorflow.keras.applications import Xception, ResNet50V2, EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\nfrom tensorflow.keras.applications import MobileNet\nimport tensorflow as tf\n\n# dog breed 종류는 120가지\nN_CLASSES = 120\n\ndef create_model(model_type = 'xception', in_shape = (224, 224, 3), n_classes = 120):\n    input_tensor = Input(shape = in_shape)\n\n    if model_type == 'resnet50v2':\n        base_model = tf.keras.applications.ResNet50V2(include_top = False, weights = 'imagenet', input_tensor = input_tensor)\n    elif model_type == 'xception':\n        base_model = tf.keras.applications.Xception(include_top = False, weights ='imagenet', input_tensor = input_tensor)\n    elif model_type == 'efficientnetb0':\n        base_model = tf.keras.applications.EfficientNetB0(include_top = False, weights ='imagenet', input_tensor = input_tensor)\n    elif model_type == 'efficientnetb1': \n        base_model = tf.keras.applications.EfficientNetB1(include_top = False, weights ='imagenet', input_tensor = input_tensor)\n    elif model_type == 'efficientnetb2':\n        base_model = tf.keras.applications.EfficientNetB2(include_top = False, weights ='imagenet', input_tensor = input_tensor)\n    elif model_type == 'efficientnetb3':\n        base_model = tf.keras.applications.EfficientNetB3(include_top = False, weights = 'imagenet', input_tensor = input_tensor)\n        \n    x = base_model.output  \n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation = 'relu')(x)\n    x = Dropout(0.5)(x)    \n    preds = Dense(units = n_classes, activation = 'softmax')(x)\n    model = Model(inputs = input_tensor, outputs = preds)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:26:10.754189Z","iopub.execute_input":"2022-02-02T15:26:10.754487Z","iopub.status.idle":"2022-02-02T15:26:10.766969Z","shell.execute_reply.started":"2022-02-02T15:26:10.754458Z","shell.execute_reply":"2022-02-02T15:26:10.766287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import Sequence\nimport sklearn \nimport cv2\n\nimport albumentations as A\n\nIMAGE_DIR = '/kaggle/working/Images'\n\ndef make_dogbreed_dataframe(image_dir = IMAGE_DIR):\n    paths = []\n    label_gubuns = []\n    for dirname, _, filenames in os.walk(image_dir):\n        for filename in filenames:\n            # 이미지 파일이 아닌 파일도 해당 디렉토리에 있음.\n            if '.jpg' in filename:\n                # 파일의 절대 경로를 file_path 변수에 할당. \n                file_path = dirname + '/' +  filename\n                paths.append(file_path)\n                # 이미지 파일의 절대 경로에서 레이블명 생성을 위한 1차 추출. '/'로 분할하여 파일 바로 위 서브디렉토리 이름 가져옴.  \n                start_pos = file_path.find('/', 20)\n                end_pos = file_path.rfind('/')\n                imsi_breed = file_path[start_pos + 1 : end_pos]\n                # 1차 추출된 데이터를 기반으로 '-' 이후 데이터가 레이블 값임. \n                breed = imsi_breed[imsi_breed.find('-') + 1:]\n                #print(start_pos, end_pos, imsi_breed)\n                label_gubuns.append(breed)\n\n    data_df = pd.DataFrame({'path':paths, 'label':label_gubuns})\n    return data_df\n\n# 학습과 검증 데이터용 numpy array 분리. \ndef get_train_valid(train_df, valid_size = 0.2, random_state = 2021):\n    train_path = train_df['path'].values\n    train_label = pd.get_dummies(train_df['label']).values\n    \n    tr_path, val_path, tr_label, val_label = train_test_split(train_path, train_label, test_size = valid_size, random_state = random_state)\n    print('tr_path shape:', tr_path.shape, 'tr_label shape:', tr_label.shape, 'val_path shape:', val_path.shape, 'val_label shape:', val_label.shape)\n    return tr_path, val_path, tr_label, val_label\n\n\n# 입력 인자 image_filenames, labels는 모두 numpy array로 들어옴. \nclass Breed_Dataset(Sequence):\n    def __init__(self, image_filenames, labels, image_size = 224, batch_size = 64, \n                 augmentor = None, shuffle = False, pre_func = None):\n        '''\n        파라미터 설명\n        image_filenames: opencv로 image를 로드할 파일의 절대 경로들\n        labels: 해당 image의 label들\n        batch_size: __getitem__(self, index) 호출 시 마다 가져올 데이터 batch 건수\n        augmentor: albumentations 객체\n        shuffle: 학습 데이터의 경우 epoch 종료시마다 데이터를 섞을지 여부\n        '''\n        # 객체 생성 인자로 들어온 값을 객체 내부 변수로 할당. \n        self.image_filenames = image_filenames\n        self.labels = labels\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.augmentor = augmentor\n        self.pre_func = pre_func\n        # train data의 경우 \n        self.shuffle = shuffle\n        if self.shuffle:\n            # 객체 생성시에 한번 데이터를 섞음. \n            #self.on_epoch_end()\n            pass\n    \n    # Sequence를 상속받은 Dataset은 batch_size 단위로 입력된 데이터를 처리함. \n    # __len__()은 전체 데이터 건수가 주어졌을 때 batch_size단위로 몇번 데이터를 반환하는지 나타남\n    def __len__(self):\n        # batch_size단위로 데이터를 몇번 가져와야하는지 계산하기 위해 전체 데이터 건수를 batch_size로 나누되, 정수로 정확히 나눠지지 않을 경우 1회를 더한다. \n        return int(np.ceil(len(self.labels) / self.batch_size))\n    \n    # batch_size 단위로 image_array, label_array 데이터를 가져와서 변환한 뒤 다시 반환함\n    # 인자로 몇번째 batch 인지를 나타내는 index를 입력하면 해당 순서에 해당하는 batch_size 만큼의 데이타를 가공하여 반환\n    # batch_size 갯수만큼 변환된 image_array와 label_array 반환. \n    def __getitem__(self, index):\n        # index는 몇번째 batch인지를 나타냄. \n        # batch_size만큼 순차적으로 데이터를 가져오려면 array에서 index * self.batch_size : (index + 1) * self.batch_size 만큼의 연속 데이터를 가져오면 됨\n        image_name_batch = self.image_filenames[index * self.batch_size : (index + 1) * self.batch_size]\n        if self.labels is not None:\n            label_batch = self.labels[index * self.batch_size : (index + 1) * self.batch_size]\n        \n        # 만일 객체 생성 인자로 albumentation으로 만든 augmentor가 주어진다면 아래와 같이 augmentor를 이용하여 image 변환\n        # albumentations은 개별 image만 변환할 수 있으므로 batch_size만큼 할당된 image_name_batch를 한 건씩 iteration하면서 변환 수행. \n        # image_batch 배열은 float32 로 설정. \n        image_batch = np.zeros((image_name_batch.shape[0], self.image_size, self.image_size, 3), dtype = 'float32')\n        \n        # batch_size에 담긴 건수만큼 iteration 하면서 opencv image load -> image augmentation 변환(augmentor가 not None일 경우)-> image_batch에 담음. \n        for image_index in range(image_name_batch.shape[0]):\n            image = cv2.cvtColor(cv2.imread(image_name_batch[image_index]), cv2.COLOR_BGR2RGB)\n            if self.augmentor is not None:\n                image = self.augmentor(image=image)['image']\n            #crop 시 잘린 이미지가 원본 이미지와 다르게 되므로 augmentation 적용 후 resize() 적용. \n            image = cv2.resize(image, (self.image_size, self.image_size))\n            # 만일 preprocessing_input이 pre_func인자로 들어오면 이를 이용하여 scaling 적용. \n            if self.pre_func is not None:\n                image = self.pre_func(image)\n                \n            image_batch[image_index] = image\n        \n        return image_batch, label_batch\n    \n    # epoch가 한번 수행이 완료 될 때마다 모델의 fit()에서 호출됨. \n    def on_epoch_end(self):\n        if(self.shuffle):\n            #print('epoch end')\n            # 전체 image 파일의 위치와 label를 쌍을 맞춰서 섞어준다. scikt learn의 utils.shuffle에서 해당 기능 제공\n            self.image_filenames, self.labels = sklearn.utils.shuffle(self.image_filenames, self.labels)\n        else:\n            pass\n        \n        \naugmentor_light = A.Compose([\n    A.HorizontalFlip(p = 0.5),\n])","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:26:14.314826Z","iopub.execute_input":"2022-02-02T15:26:14.315083Z","iopub.status.idle":"2022-02-02T15:26:14.337386Z","shell.execute_reply.started":"2022-02-02T15:26:14.315052Z","shell.execute_reply":"2022-02-02T15:26:14.336456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### train_model 함수를 Config 클래스 값을 인자로 입력 받을 수 있도록 변경. ","metadata":{}},{"cell_type":"code","source":"def train_model(train_df, config = Config):\n    # 학습과 검증 데이터 이미지/레이블로 분리하고 학습/검증 Dataset 생성. \n    tr_path, val_path, tr_label, val_label = get_train_valid(train_df, valid_size = 0.2, random_state = 2021)\n    \n    tr_ds = Breed_Dataset(tr_path, tr_label, image_size = config.IMAGE_SIZE, batch_size = config.BATCH_SIZE, \n                          augmentor = config.AUGMENTOR, shuffle = True, pre_func = config.PRE_FUNC)\n    val_ds = Breed_Dataset(val_path, val_label, image_size = config.IMAGE_SIZE, batch_size = config.BATCH_SIZE, \n                          augmentor = None, shuffle = False, pre_func = config.PRE_FUNC)\n    if config.DEBUG:\n        tr_image_batch = next(iter(tr_ds))[0]\n        val_image_batch = next(iter(val_ds))[0]\n        print(tr_image_batch.shape, val_image_batch.shape)\n        print(tr_image_batch[0], val_image_batch[0])\n        \n    # model_type인자로 들어온 모델 생성. optimizer Adam적용. \n    print('#######', config.MODEL_TYPE, ' 생성 및 학습 수행 ########')\n    model = create_model(model_type = config.MODEL_TYPE, in_shape = (config.IMAGE_SIZE, config.IMAGE_SIZE, 3), n_classes = 120)\n    model.compile(optimizer = Adam(lr = config.INITIAL_LR), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    # 만일 Fine tuning 일 경우 아래 로직 적용. \n    if config.IS_FINE_TUNING:\n        print('####### Fine tuning 학습을 시작합니다. ########')\n        # 첫번째 Fine Tuning. Feature Extractor를 제외한 classification layer를 학습.(Feature Extractor layer들을 trainable=False 설정)\n        for layer in model.layers[:-4]:\n            layer.trainable = False\n        \n        print('####### Classification Layer들의 학습을 시작합니다. ########')\n        history = model.fit(tr_ds, epochs = config.FIRST_EPOCHS, steps_per_epoch = int(np.ceil(tr_path.shape[0] / config.BATCH_SIZE)), \n                           validation_data = val_ds, validation_steps = int(np.ceil(val_path.shape[0] / config.BATCH_SIZE)),\n                           callbacks = (config.FIRST_CALLBACKS), verbose = 1)\n        \n        # 두번째, 전체 Layer를 학습. 전체 layer를 trainable=True로 수정. Batch Normalization layer는 fine tuning시 계속 trainable=False 설정. \n        for layer in model.layers:\n            if not isinstance(layer, layers.BatchNormalization):\n                layer.trainable = True\n        \n        print('####### 전체 Layer들의 학습을 시작합니다. ########')\n        history = model.fit(tr_ds, epochs = config.SECOND_EPOCHS, steps_per_epoch = int(np.ceil(tr_path.shape[0] / config.BATCH_SIZE)), \n                           validation_data = val_ds, validation_steps = int(np.ceil(val_path.shape[0] / config.BATCH_SIZE)),\n                           callbacks = (config.SECOND_CALLBACKS), verbose = 1)\n    \n    # Fine Tuning이 아닐 경우 \n    else:\n        print('####### 학습을 시작합니다. ########')\n        history = model.fit(tr_ds, epochs = config.N_EPOCHS, steps_per_epoch = int(np.ceil(tr_path.shape[0] / config.BATCH_SIZE)), \n                       validation_data = val_ds, validation_steps = int(np.ceil(val_path.shape[0] / config.BATCH_SIZE)),\n                       callbacks = (config.FIRST_CALLBACKS), verbose = 1)\n        \n    return model, history\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:26:20.617032Z","iopub.execute_input":"2022-02-02T15:26:20.61737Z","iopub.status.idle":"2022-02-02T15:26:20.633024Z","shell.execute_reply.started":"2022-02-02T15:26:20.617335Z","shell.execute_reply":"2022-02-02T15:26:20.632297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EfficientNetB1 모델 학습 및 성능 평가\n* Config 클래스의 내부 변수값으로 할당 될 수 있도록 Learning Rate Scheduler에 적용될 함수, Callback 객체, Augmentation객체들을 생성\n* EfficientNetB1의 경우 240x240 이미지 크기로 최적화 되었으므로 이에 맞게 Config 값 설정. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\nimport tensorflow as tf\n\n# learning rate scheduler에 적용할 함수 선언. \ndef lrfn_01(epoch):\n    LR_START = 1e-5\n    LR_MAX = 1e-4\n    LR_RAMPUP_EPOCHS = 2\n    LR_SUSTAIN_EPOCHS = 1\n    LR_STEP_DECAY = 0.75\n    \n    def calc_fn(epoch):\n        if epoch < LR_RAMPUP_EPOCHS:\n            lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n            lr = LR_MAX\n        else:\n            lr = LR_MAX * LR_STEP_DECAY ** ((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) // 2)\n        return lr\n    \n    return calc_fn(epoch)\n\ndef lrfn_02(epoch):\n    LR_START = 1e-6\n    LR_MAX = 2e-5\n    LR_RAMPUP_EPOCHS = 2\n    LR_SUSTAIN_EPOCHS = 1\n    LR_STEP_DECAY = 0.75\n    \n    def calc_fn(epoch):\n        if epoch < LR_RAMPUP_EPOCHS:\n            lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n        elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n            lr = LR_MAX\n        else:\n            lr = LR_MAX * LR_STEP_DECAY ** ((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) // 2)\n        return lr\n    \n    return calc_fn(epoch)\n\n# Config에 입력할 callback 생성. \nlr01_cb = tf.keras.callbacks.LearningRateScheduler(lrfn_01, verbose = 1)\nlr02_cb = tf.keras.callbacks.LearningRateScheduler(lrfn_02, verbose = 1)\nely_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, mode = 'min', verbose = 1)\n\n# Augmentor 생성. \n\naugmentor_light_02 = A.Compose([\n    A.HorizontalFlip(p = 0.3),\n    A.ShiftScaleRotate(scale_limit = (0.7, 0.9), p = 0.2, rotate_limit = 30),\n    A.RandomBrightnessContrast(brightness_limit = (-0.2, 0.2), contrast_limit = (-0.2, 0.2), p = 0.2),\n    A.ColorJitter(p = 0.2)\n])\n\n# Config 생성. \nclass Config:\n    MODEL_TYPE = 'efficientnetb1'\n    IMAGE_SIZE = 240\n    BATCH_SIZE = 64\n    N_EPOCHS = 30 # fine tuning이 아닐 경우 전체 수행 epoch 횟수\n    IS_FINE_TUNING = True\n    FIRST_EPOCHS = 15 # fine tuning 일 경우 첫번째 epoch 횟수\n    SECOND_EPOCHS = 15 # fine tuning 일 경우 두번째 epoch 횟수\n    FIRST_CALLBACKS = [lr01_cb, ely_cb] #모델 train시 적용될 callback 객체 리스트\n    SECOND_CALLBACKS = [lr02_cb, ely_cb] #만일 Fine tuning 시 첫번째 학습과 두번째 학습의 Learning rate scheduler가 서로 다를 경우 사용. \n    AUGMENTOR = augmentor_light_02\n    PRE_FUNC = eff_preprocess_input\n    INITIAL_LR = 0.0001\n    DEBUG = False","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:26:25.530484Z","iopub.execute_input":"2022-02-02T15:26:25.530945Z","iopub.status.idle":"2022-02-02T15:26:25.545906Z","shell.execute_reply.started":"2022-02-02T15:26:25.530907Z","shell.execute_reply":"2022-02-02T15:26:25.545093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EfficientNetB1 모델 학습. \neff1_model, history = train_model(train_df, config = Config)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:26:31.126329Z","iopub.execute_input":"2022-02-02T15:26:31.127057Z","iopub.status.idle":"2022-02-02T16:22:11.499511Z","shell.execute_reply.started":"2022-02-02T15:26:31.127012Z","shell.execute_reply":"2022-02-02T16:22:11.498701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, test_df, config = Config):\n    test_path = test_df['path'].values\n    test_label = pd.get_dummies(test_df['label']).values\n    test_ds = Breed_Dataset(test_path, test_label, image_size = config.IMAGE_SIZE, batch_size = config.BATCH_SIZE, \n                        augmentor = None, shuffle = False, pre_func = config.PRE_FUNC)\n\n    evaluation_result = model.evaluate(test_ds)\n    print('evaluation_result:', evaluation_result)\n    \n    return model, evaluation_result\n \nmodel, evaluation_result = evaluate_model(model=eff1_model, test_df = test_df, config = Config)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-02T16:25:03.515286Z","iopub.execute_input":"2022-02-02T16:25:03.515903Z","iopub.status.idle":"2022-02-02T16:25:45.396913Z","shell.execute_reply.started":"2022-02-02T16:25:03.515863Z","shell.execute_reply":"2022-02-02T16:25:45.395696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}